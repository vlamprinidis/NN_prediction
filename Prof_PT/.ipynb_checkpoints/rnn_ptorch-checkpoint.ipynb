{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "world_size = 3\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallelCPU as DDP\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = '10.0.1.121'\n",
    "    os.environ['MASTER_PORT'] = '8890'\n",
    "    os.environ['GLOO_SOCKET_IFNAME'] = 'ens3'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(backend='gloo', \n",
    "                            init_method='env://', rank=rank, world_size=world_size)\n",
    "\n",
    "    # Explicitly setting seed to make sure that models created in two processes\n",
    "    # start from same random weights and biases.\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "setup(rank = rank, world_size = world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 100\n",
    "num_epochs = 3\n",
    "learning_rate = 0.01\n",
    "\n",
    "num_classes = 10\n",
    "num_cells = 128\n",
    "dense_size = 32\n",
    "drop_pr = 0.2\n",
    "\n",
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, num_cells, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(128, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size, num_classes)\n",
    "        self.dropout = nn.Dropout(drop_pr)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(1, x.size(0), num_cells)\n",
    "        c0 = torch.zeros(1, x.size(0), num_cells)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.rnn(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, num_cells)\n",
    "        \n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        \n",
    "        out = F.relu(self.fc1(out))\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out) # no softmax needed - nn.CrossEntropy does it\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/lib/python3.7/site-packages/torch/nn/parallel/__init__.py:12: UserWarning: torch.nn.parallel.DistributedDataParallelCPU is deprecated, please use torch.nn.parallel.DistributedDataParallel instead.\n",
      "  warnings.warn(\"torch.nn.parallel.DistributedDataParallelCPU is deprecated, \"\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "#                                            shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "model = RNN(input_size)\n",
    "\n",
    "model = DDP(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train():\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.reshape(-1, sequence_length, input_size)\n",
    "            labels = labels\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "\n",
    "def test():\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, sequence_length, input_size)\n",
    "            labels = labels\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/200], Loss: 0.7752\n",
      "Epoch [1/3], Step [200/200], Loss: 0.3188\n",
      "Epoch [2/3], Step [100/200], Loss: 0.2291\n",
      "Epoch [2/3], Step [200/200], Loss: 0.1822\n",
      "Epoch [3/3], Step [100/200], Loss: 0.1680\n",
      "Epoch [3/3], Step [200/200], Loss: 0.2278\n",
      "-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "Name                                 Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  \n",
      "-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "random_                              0.00%            82.447us         0.00%            82.447us         27.482us         3                \n",
      "is_floating_point                    0.00%            13.754us         0.00%            13.754us         1.528us          9                \n",
      "is_complex                           0.00%            3.384us          0.00%            3.384us          1.128us          3                \n",
      "item                                 0.27%            140.184ms        0.49%            256.561ms        4.275us          60009            \n",
      "_local_scalar_dense                  0.22%            116.377ms        0.22%            116.377ms        1.939us          60009            \n",
      "select                               1.21%            638.340ms        1.21%            638.340ms        5.267us          121200           \n",
      "empty                                1.03%            542.695ms        1.03%            542.695ms        4.391us          123600           \n",
      "set_                                 0.57%            300.699ms        0.57%            300.699ms        5.012us          60000            \n",
      "view                                 1.72%            909.216ms        1.72%            909.216ms        10.102us         90000            \n",
      "transpose                            1.20%            631.301ms        1.20%            631.301ms        5.183us          121800           \n",
      "to                                   1.39%            731.337ms        1.86%            981.913ms        16.203us         60600            \n",
      "div                                  2.28%            1.203s           2.28%            1.203s           18.569us         64800            \n",
      "stack                                3.51%            1.849s           3.51%            1.849s           616.424us        3000             \n",
      "detach_                              0.02%            8.957ms          0.02%            8.957ms          1.661us          5392             \n",
      "reshape                              0.01%            7.141ms          0.04%            21.983ms         18.319us         1200             \n",
      "lstm                                 1.06%            559.223ms        37.29%           19.666s          32.776ms         600              \n",
      "cudnn_is_acceptable                  0.00%            1.590ms          0.00%            1.590ms          2.649us          600              \n",
      "unbind                               0.22%            117.496ms        0.22%            117.496ms        48.957us         2400             \n",
      "linear                               0.22%            115.982ms        15.61%           8.233s           473.173us        17400            \n",
      "unsigned short                       1.55%            814.840ms        1.55%            814.840ms        8.876us          91800            \n",
      "matmul                               0.03%            15.417ms         3.05%            1.610s           2.683ms          600              \n",
      "contiguous                           0.38%            199.281ms        0.39%            204.586ms        340.976us        600              \n",
      "empty_like                           0.02%            9.887ms          0.04%            20.746ms         11.526us         1800             \n",
      "mm                                   23.65%           12.473s          23.65%           12.473s          340.804us        36600            \n",
      "_unsafe_view                         0.03%            14.094ms         0.03%            14.094ms         23.489us         600              \n",
      "add_                                 3.01%            1.590s           3.01%            1.590s           32.713us         48592            \n",
      "addmm                                11.69%           6.163s           11.69%           6.163s           342.379us        18000            \n",
      "chunk                                0.12%            62.391ms         0.74%            390.577ms        23.249us         16800            \n",
      "split                                0.62%            328.186ms        0.62%            328.186ms        19.535us         16800            \n",
      "sigmoid_                             5.23%            2.760s           5.23%            2.760s           54.770us         50400            \n",
      "tanh_                                3.24%            1.710s           3.24%            1.710s           101.759us        16800            \n",
      "mul                                  7.68%            4.047s           7.68%            4.047s           26.453us         153000           \n",
      "tanh                                 4.93%            2.602s           4.93%            2.602s           154.878us        16800            \n",
      "slice                                0.22%            113.632ms        0.22%            113.632ms        9.469us          12000            \n",
      "dropout                              0.03%            17.478ms         0.44%            230.777ms        192.314us        1200             \n",
      "bernoulli_                           0.23%            118.980ms        0.23%            118.980ms        99.150us         1200             \n",
      "div_                                 0.36%            189.772ms        0.36%            189.772ms        105.429us        1800             \n",
      "relu                                 0.03%            17.836ms         0.03%            17.836ms         29.726us         600              \n",
      "log_softmax                          0.01%            4.076ms          0.08%            44.241ms         73.735us         600              \n",
      "_log_softmax                         0.08%            40.165ms         0.08%            40.165ms         66.942us         600              \n",
      "nll_loss                             0.01%            4.931ms          0.04%            23.195ms         38.659us         600              \n",
      "nll_loss_forward                     0.03%            18.264ms         0.03%            18.264ms         30.441us         600              \n",
      "torch::autograd::GraphRoot           0.00%            1.940ms          0.00%            1.940ms          3.233us          600              \n",
      "NllLossBackward                      0.03%            15.574ms         0.08%            44.118ms         73.529us         600              \n",
      "nll_loss_backward                    0.05%            28.544ms         0.05%            28.544ms         47.573us         600              \n",
      "LogSoftmaxBackward                   0.01%            7.676ms          0.07%            38.185ms         63.641us         600              \n",
      "_log_softmax_backward_data           0.06%            30.509ms         0.06%            30.509ms         50.848us         600              \n",
      "AddmmBackward                        0.71%            376.872ms        21.00%           11.072s          615.134us        18000            \n",
      "sum                                  2.61%            1.375s           2.61%            1.375s           73.905us         18600            \n",
      "torch::autograd::AccumulateGrad      0.05%            23.924ms         0.18%            93.726ms         19.526us         4800             \n",
      "detach                               0.00%            10.068us         0.00%            10.068us         1.259us          8                \n",
      "narrow                               0.07%            39.176ms         0.25%            131.537ms        13.702us         9600             \n",
      "TBackward                            0.12%            65.679ms         0.36%            191.962ms        10.321us         18600            \n",
      "MulBackward0                         0.99%            523.236ms        5.19%            2.734s           52.994us         51600            \n",
      "ReluBackward0                        0.01%            4.756ms          0.03%            14.457ms         24.095us         600              \n",
      "threshold_backward                   0.02%            9.701ms          0.02%            9.701ms          16.169us         600              \n",
      "SliceBackward                        0.20%            104.511ms        0.47%            248.944ms        207.454us        1200             \n",
      "zeros                                0.02%            9.003ms          0.50%            262.273ms        145.707us        1800             \n",
      "zero_                                0.69%            364.726ms        0.69%            364.726ms        55.329us         6592             \n",
      "SelectBackward                       0.04%            19.371ms         0.29%            153.526ms        255.877us        600              \n",
      "TransposeBackward0                   0.00%            2.611ms          0.01%            7.565ms          12.608us         600              \n",
      "StackBackward                        0.01%            3.719ms          0.11%            56.000ms         93.333us         600              \n",
      "TanhBackward                         0.41%            214.006ms        6.29%            3.318s           98.763us         33600            \n",
      "tanh_backward                        5.89%            3.104s           5.89%            3.104s           92.394us         33600            \n",
      "AddBackward0                         0.12%            62.617ms         0.12%            62.617ms         1.831us          34200            \n",
      "SigmoidBackward                      0.61%            324.061ms        2.41%            1.272s           25.232us         50400            \n",
      "sigmoid_backward                     1.80%            947.628ms        1.80%            947.628ms        18.802us         50400            \n",
      "SplitBackward                        0.27%            143.201ms        2.43%            1.280s           76.193us         16800            \n",
      "cat                                  2.16%            1.137s           2.16%            1.137s           67.669us         16800            \n",
      "add                                  3.68%            1.941s           3.68%            1.941s           29.959us         64800            \n",
      "UnbindBackward                       0.01%            5.782ms          2.39%            1.258s           2.097ms          600              \n",
      "UnsafeViewBackward                   0.01%            3.790ms          0.02%            10.088ms         16.814us         600              \n",
      "MmBackward                           0.02%            11.003ms         1.70%            897.359ms        1.496ms          600              \n",
      "mul_                                 0.38%            200.990ms        0.38%            200.990ms        20.936us         9600             \n",
      "addcmul_                             0.12%            65.433ms         0.12%            65.433ms         13.632us         4800             \n",
      "sqrt                                 0.55%            290.305ms        0.55%            290.305ms        60.480us         4800             \n",
      "addcdiv_                             0.17%            88.329ms         0.17%            88.329ms         18.402us         4800             \n",
      "-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "Self CPU time total: 52.733s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.47 %\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "        train()\n",
    "#     tbl = prof.key_averages().table(sort_by=\"self_cpu_time_total\")\n",
    "#     tbl = prof.key_averages().table()\n",
    "    tbl = prof.table()\n",
    "    print(tbl)\n",
    "    \n",
    "    test()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# profs = []\n",
    "# # Train the model\n",
    "# total_step = len(train_loader)\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "#         images = images.reshape(-1, sequence_length, input_size)\n",
    "#         labels = labels\n",
    "\n",
    "#         # Forward pass\n",
    "#         with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "#             outputs = model(images)\n",
    "#         tbl = prof.key_averages().table(sort_by=\"self_cpu_time_total\")\n",
    "#         profs.append(tbl)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (i+1) % 100 == 0:\n",
    "#             print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                    .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
