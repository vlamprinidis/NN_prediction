{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "world_size = 3\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallelCPU as DDP\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = '10.0.1.121'\n",
    "    os.environ['MASTER_PORT'] = '8890'\n",
    "    os.environ['GLOO_SOCKET_IFNAME'] = 'ens3'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(backend='gloo', \n",
    "                            init_method='env://', rank=rank, world_size=world_size)\n",
    "\n",
    "    # Explicitly setting seed to make sure that models created in two processes\n",
    "    # start from same random weights and biases.\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "setup(rank = rank, world_size = world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 100\n",
    "num_epochs = 3\n",
    "learning_rate = 0.01\n",
    "\n",
    "num_classes = 10\n",
    "num_cells = 128\n",
    "dense_size = 32\n",
    "drop_pr = 0.2\n",
    "\n",
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, num_cells, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(128, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size, num_classes)\n",
    "        self.dropout = nn.Dropout(drop_pr)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(1, x.size(0), num_cells)\n",
    "        c0 = torch.zeros(1, x.size(0), num_cells)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.rnn(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, num_cells)\n",
    "        \n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        \n",
    "        out = F.relu(self.fc1(out))\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out) # no softmax needed - nn.CrossEntropy does it\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/lib/python3.7/site-packages/torch/nn/parallel/__init__.py:12: UserWarning: torch.nn.parallel.DistributedDataParallelCPU is deprecated, please use torch.nn.parallel.DistributedDataParallel instead.\n",
      "  warnings.warn(\"torch.nn.parallel.DistributedDataParallelCPU is deprecated, \"\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "#                                            shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "model = RNN(input_size)\n",
    "\n",
    "model = DDP(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train():\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.reshape(-1, sequence_length, input_size)\n",
    "            labels = labels\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "\n",
    "def test():\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, sequence_length, input_size)\n",
    "            labels = labels\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/200], Loss: 0.0835\n",
      "Epoch [1/3], Step [200/200], Loss: 0.1330\n",
      "Epoch [2/3], Step [100/200], Loss: 0.0809\n",
      "Epoch [2/3], Step [200/200], Loss: 0.0766\n",
      "Epoch [3/3], Step [100/200], Loss: 0.0340\n",
      "Epoch [3/3], Step [200/200], Loss: 0.0616\n",
      "-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "Name                                 Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  \n",
      "-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "random_                              0.00%            103.054us        0.00%            103.054us        34.351us         3                \n",
      "is_floating_point                    0.00%            7.424us          0.00%            7.424us          0.825us          9                \n",
      "is_complex                           0.00%            1.168us          0.00%            1.168us          0.389us          3                \n",
      "item                                 0.19%            98.446ms         0.40%            207.586ms        3.459us          60009            \n",
      "_local_scalar_dense                  0.21%            109.140ms        0.21%            109.140ms        1.819us          60009            \n",
      "select                               1.23%            631.396ms        1.23%            631.396ms        5.210us          121200           \n",
      "empty                                1.00%            515.741ms        1.00%            515.741ms        4.214us          122400           \n",
      "set_                                 0.57%            293.359ms        0.57%            293.359ms        4.889us          60000            \n",
      "view                                 1.71%            881.825ms        1.71%            881.825ms        9.798us          90000            \n",
      "transpose                            1.20%            617.142ms        1.20%            617.142ms        5.067us          121800           \n",
      "to                                   1.38%            710.927ms        1.85%            949.506ms        15.668us         60600            \n",
      "div                                  2.33%            1.198s           2.33%            1.198s           18.493us         64800            \n",
      "stack                                3.42%            1.757s           3.42%            1.757s           585.580us        3000             \n",
      "detach_                              0.01%            4.379ms          0.01%            4.379ms          0.811us          5400             \n",
      "reshape                              0.01%            4.954ms          0.04%            19.903ms         16.586us         1200             \n",
      "lstm                                 0.78%            401.405ms        37.57%           19.320s          32.200ms         600              \n",
      "cudnn_is_acceptable                  0.00%            1.273ms          0.00%            1.273ms          2.122us          600              \n",
      "unbind                               0.24%            121.514ms        0.24%            121.514ms        50.631us         2400             \n",
      "linear                               0.15%            77.747ms         15.82%           8.133s           467.404us        17400            \n",
      "unsigned short                       1.43%            733.382ms        1.43%            733.382ms        7.989us          91800            \n",
      "matmul                               0.03%            14.240ms         3.09%            1.588s           2.647ms          600              \n",
      "contiguous                           0.39%            198.697ms        0.40%            204.096ms        340.160us        600              \n",
      "empty_like                           0.00%            2.030ms          0.01%            5.399ms          8.999us          600              \n",
      "mm                                   24.15%           12.421s          24.15%           12.421s          339.359us        36600            \n",
      "_unsafe_view                         0.03%            13.343ms         0.03%            13.343ms         22.238us         600              \n",
      "add_                                 3.15%            1.621s           3.15%            1.621s           33.363us         48600            \n",
      "addmm                                11.96%           6.149s           11.96%           6.149s           341.606us        18000            \n",
      "chunk                                0.08%            40.992ms         0.72%            369.476ms        21.993us         16800            \n",
      "split                                0.64%            328.485ms        0.64%            328.485ms        19.553us         16800            \n",
      "sigmoid_                             5.19%            2.667s           5.19%            2.667s           52.909us         50400            \n",
      "tanh_                                3.29%            1.691s           3.29%            1.691s           100.636us        16800            \n",
      "mul                                  7.63%            3.922s           7.63%            3.922s           26.040us         150600           \n",
      "tanh                                 5.18%            2.663s           5.18%            2.663s           158.519us        16800            \n",
      "slice                                0.19%            98.581ms         0.19%            98.581ms         8.215us          12000            \n",
      "dropout                              0.01%            2.599ms          0.01%            2.599ms          2.166us          1200             \n",
      "relu                                 0.04%            18.503ms         0.04%            18.503ms         30.839us         600              \n",
      "log_softmax                          0.01%            3.156ms          0.08%            43.531ms         72.552us         600              \n",
      "_log_softmax                         0.08%            40.376ms         0.08%            40.376ms         67.293us         600              \n",
      "nll_loss                             0.01%            3.110ms          0.04%            19.986ms         33.310us         600              \n",
      "nll_loss_forward                     0.03%            16.876ms         0.03%            16.876ms         28.127us         600              \n",
      "zero_                                0.74%            381.904ms        0.74%            381.904ms        57.864us         6600             \n",
      "torch::autograd::GraphRoot           0.00%            1.336ms          0.00%            1.336ms          2.226us          600              \n",
      "NllLossBackward                      0.03%            15.260ms         0.09%            45.442ms         75.736us         600              \n",
      "nll_loss_backward                    0.06%            30.181ms         0.06%            30.181ms         50.302us         600              \n",
      "LogSoftmaxBackward                   0.01%            6.267ms          0.07%            36.099ms         60.165us         600              \n",
      "_log_softmax_backward_data           0.06%            29.833ms         0.06%            29.833ms         49.721us         600              \n",
      "AddmmBackward                        0.62%            316.450ms        21.23%           10.916s          606.420us        18000            \n",
      "sum                                  2.73%            1.405s           2.73%            1.405s           75.534us         18600            \n",
      "torch::autograd::AccumulateGrad      0.04%            18.117ms         0.17%            85.973ms         17.911us         4800             \n",
      "narrow                               0.05%            23.235ms         0.20%            103.528ms        10.784us         9600             \n",
      "TBackward                            0.09%            48.762ms         0.31%            160.627ms        8.636us          18600            \n",
      "ReluBackward0                        0.01%            4.677ms          0.03%            16.310ms         27.183us         600              \n",
      "threshold_backward                   0.02%            11.632ms         0.02%            11.632ms         19.387us         600              \n",
      "SliceBackward                        0.21%            110.202ms        0.50%            258.451ms        215.376us        1200             \n",
      "zeros                                0.01%            6.678ms          0.53%            273.603ms        152.002us        1800             \n",
      "SelectBackward                       0.03%            15.920ms         0.30%            156.073ms        260.122us        600              \n",
      "TransposeBackward0                   0.00%            2.001ms          0.01%            6.580ms          10.967us         600              \n",
      "StackBackward                        0.01%            3.137ms          0.11%            57.851ms         96.419us         600              \n",
      "MulBackward0                         0.88%            454.873ms        5.07%            2.606s           51.709us         50400            \n",
      "TanhBackward                         0.34%            175.491ms        6.30%            3.241s           96.456us         33600            \n",
      "tanh_backward                        5.96%            3.065s           5.96%            3.065s           91.233us         33600            \n",
      "AddBackward0                         0.09%            46.529ms         0.09%            46.529ms         1.360us          34200            \n",
      "SigmoidBackward                      0.50%            255.597ms        2.27%            1.169s           23.186us         50400            \n",
      "sigmoid_backward                     1.78%            912.979ms        1.78%            912.979ms        18.115us         50400            \n",
      "SplitBackward                        0.23%            117.835ms        2.33%            1.196s           71.220us         16800            \n",
      "cat                                  2.10%            1.079s           2.10%            1.079s           64.206us         16800            \n",
      "add                                  3.93%            2.019s           3.93%            2.019s           31.165us         64800            \n",
      "UnbindBackward                       0.01%            5.687ms          2.25%            1.158s           1.930ms          600              \n",
      "UnsafeViewBackward                   0.01%            2.921ms          0.02%            8.605ms          14.342us         600              \n",
      "MmBackward                           0.02%            10.253ms         1.77%            907.599ms        1.513ms          600              \n",
      "div_                                 0.19%            99.456ms         0.19%            99.456ms         165.760us        600              \n",
      "mul_                                 0.40%            204.499ms        0.40%            204.499ms        21.302us         9600             \n",
      "addcmul_                             0.12%            62.111ms         0.12%            62.111ms         12.940us         4800             \n",
      "sqrt                                 0.62%            320.438ms        0.62%            320.438ms        66.758us         4800             \n",
      "addcdiv_                             0.17%            86.294ms         0.17%            86.294ms         17.978us         4800             \n",
      "-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "Self CPU time total: 51.421s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.97 %\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "        train()\n",
    "#     tbl = prof.key_averages().table(sort_by=\"self_cpu_time_total\")\n",
    "    tbl = prof.key_averages().table()\n",
    "#     tbl = prof.table()\n",
    "    print(tbl)\n",
    "    \n",
    "    test()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# profs = []\n",
    "# # Train the model\n",
    "# total_step = len(train_loader)\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "#         images = images.reshape(-1, sequence_length, input_size)\n",
    "#         labels = labels\n",
    "\n",
    "#         # Forward pass\n",
    "#         with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "#             outputs = model(images)\n",
    "#         tbl = prof.key_averages().table(sort_by=\"self_cpu_time_total\")\n",
    "#         profs.append(tbl)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (i+1) % 100 == 0:\n",
    "#             print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                    .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
