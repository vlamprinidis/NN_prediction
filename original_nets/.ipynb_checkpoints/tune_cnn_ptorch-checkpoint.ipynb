{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 28\n",
    "input_size = 28\n",
    "\n",
    "BATCH = 512\n",
    "EPOCHS = 10\n",
    "\n",
    "RANK = 0\n",
    "NODES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_size_out(size_in, kern, stride):\n",
    "    pad = 0\n",
    "    size_out = (size_in + 2*pad - (kern - 1) - 1)/stride +1\n",
    "    return size_out\n",
    "\n",
    "def avg_size_out(size_in, kern, stride):\n",
    "    pad = 0\n",
    "    size_out = (size_in + 2*pad - kern)/stride +1\n",
    "    return size_out\n",
    "    \n",
    "class Cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cnn, self).__init__()\n",
    "        \n",
    "        CONV_KERNEL = 5\n",
    "        CONV_STRIDE = 1\n",
    "\n",
    "        AVG_KERNEL = 2\n",
    "        AVG_STRIDE = 2\n",
    "        \n",
    "        L1 = avg_size_out(conv_size_out(28, CONV_KERNEL, CONV_STRIDE), \n",
    "                     AVG_KERNEL, AVG_STRIDE)\n",
    "        \n",
    "        L2 = avg_size_out(conv_size_out(L1, CONV_KERNEL, CONV_STRIDE), \n",
    "                     AVG_KERNEL, AVG_STRIDE)\n",
    "        \n",
    "        LINEAR_IN = 16*(int(L2) ** 2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = 1, out_channels = 6,\n",
    "            kernel_size = CONV_KERNEL, stride = CONV_STRIDE\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 6, out_channels = 16,\n",
    "            kernel_size = CONV_KERNEL, stride = CONV_STRIDE\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.AvgPool2d(\n",
    "            kernel_size = AVG_KERNEL, stride = AVG_STRIDE\n",
    "        )\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features = LINEAR_IN,\n",
    "            out_features = 120\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features = 120,\n",
    "            out_features = 84\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Linear(\n",
    "            in_features = 84,\n",
    "            out_features = 10\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = torch.tanh(self.conv1(img))\n",
    "        out = self.pool(out)\n",
    "        \n",
    "        out = torch.tanh(self.conv2(out))\n",
    "        out = self.pool(out)\n",
    "        \n",
    "        out = self.flat(out)\n",
    "        \n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.flat(out)\n",
    "        \n",
    "        out = torch.tanh(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NODES > 1:\n",
    "    import os\n",
    "    import torch.distributed as dist\n",
    "    from torch.nn.parallel import DistributedDataParallelCPU as DDP\n",
    "\n",
    "    def setup(rank, world_size):\n",
    "        os.environ['MASTER_ADDR'] = '10.0.1.121'\n",
    "        os.environ['MASTER_PORT'] = '8890'\n",
    "        os.environ['GLOO_SOCKET_IFNAME'] = 'ens3'\n",
    "\n",
    "        # initialize the process group\n",
    "        dist.init_process_group(backend='gloo', \n",
    "                                init_method='env://', rank=rank, world_size=world_size)\n",
    "\n",
    "        # Explicitly setting seed to make sure that models created in two processes\n",
    "        # start from same random weights and biases.\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "\n",
    "    def cleanup():\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "    setup(rank = RANK, world_size = NODES)\n",
    "    \n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "                train_dataset,\n",
    "                num_replicas = NODES,\n",
    "                rank = RANK\n",
    "            )\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH,\n",
    "                                           sampler = train_sampler)\n",
    "    \n",
    "    model = DDP(Cnn())\n",
    "    \n",
    "else:\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH,\n",
    "                                           shuffle=True)\n",
    "    model = Cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter('logs/fit/torch_cnn_64_n1of1_cpu4')\n",
    "\n",
    "# dataiter = iter(train_loader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# writer.add_graph(model, images)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "def train():\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    print(total_step)\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            labels = labels\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "#             if (i+1) % 100 == 0:\n",
    "#                 print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                        .format(epoch+1, EPOCHS, i+1, total_step, loss.item()))\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, EPOCHS, loss.item()))\n",
    "\n",
    "def test():\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            labels = labels\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.autograd.profiler.profile() as prof:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ptorch_cnn_{}batch_node{}of{}_4CPUs'.format(BATCH,RANK+1,NODES)\n",
    "full_name = 'ptorch_csv/{}.csv'.format(name)\n",
    "\n",
    "helper.save_to_csv(prof.key_averages(),full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prof.key_averages()[0].cpu_time\n",
    "# prof.key_averages()[0].cpu_time_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from csvsort import csvsort\n",
    "# csvsort('ptorch_csv/{}.csv'.format(full_name), [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
