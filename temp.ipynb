{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_size_out(size_in, kern, stride):\n",
    "    pad = 0\n",
    "    dilation = 1\n",
    "    return (size_in + 2*pad - dilation*(kern - 1) - 1) // stride + 1\n",
    "\n",
    "def avg_size_out(size_in, kern, stride):\n",
    "    pad = 0\n",
    "    return (size_in + 2*pad - kern) // stride + 1\n",
    "\n",
    "def max_size_out(size_in, kern, stride):\n",
    "    pad = 0\n",
    "    dilation = 1\n",
    "    return (size_in + 2*pad - dilation*(kern - 1) - 1) // stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(            \n",
    "#             nn.MaxPool2d(4,2),\n",
    "            nn.Flatten(),\n",
    "#             nn.Linear(in_features=225, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(100,1,32,32)\n",
    "y = torch.randint(low=0,high=9,size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(x, y)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset = train_dataset,\n",
    "        batch_size = 32,\n",
    "        shuffle = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile(model, train_loader):    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "    \n",
    "    def train(epochs):\n",
    "        total_step = len(train_loader)\n",
    "        print(total_step)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                # Forward pass\n",
    "                outputs = model(images)            \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print ('Epoch [{}/{}], Step [{}/{}], Loss: {}' \n",
    "                           .format(epoch+1, epochs, i+1, total_step, loss))\n",
    "    \n",
    "    EPOCHS = 1\n",
    "    with torch.autograd.profiler.profile() as prof:\n",
    "        train(EPOCHS)\n",
    "        \n",
    "    return prof.key_averages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-cc1e088cb582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-adfeb7d90933>\u001b[0m in \u001b[0;36mprofile\u001b[0;34m(model, train_loader)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Diploma/gits/.env/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Diploma/gits/.env/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "ls = profile(Model().model,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "Name                                  Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  \n",
      "------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "aten::empty                           0.07%            269.152us        0.07%            269.152us        5.277us          51               \n",
      "aten::random_                         0.01%            50.829us         0.01%            50.829us         25.415us         2                \n",
      "aten::is_floating_point               0.01%            49.130us         0.01%            49.130us         24.565us         2                \n",
      "aten::item                            7.39%            27.771ms         16.21%           60.964ms         30.482ms         2                \n",
      "aten::_local_scalar_dense             8.83%            33.194ms         8.83%            33.194ms         16.597ms         2                \n",
      "aten::randperm                        3.13%            11.787ms         6.28%            23.618ms         11.809ms         2                \n",
      "aten::scalar_tensor                   0.00%            15.868us         0.00%            15.868us         15.868us         1                \n",
      "aten::resize_                         0.04%            160.967us        0.04%            160.967us        5.366us          30               \n",
      "aten::stride                          0.00%            15.635us         0.00%            15.635us         0.920us          17               \n",
      "aten::select                          7.18%            27.009ms         7.24%            27.226ms         136.130us        200              \n",
      "aten::as_strided                      0.14%            544.956us        0.14%            544.956us        1.276us          427              \n",
      "aten::stack                           0.14%            509.920us        7.87%            29.608ms         3.701ms          8                \n",
      "aten::unsqueeze                       2.41%            9.049ms          2.47%            9.303ms          46.515us         200              \n",
      "aten::cat                             0.12%            467.038us        5.26%            19.795ms         2.474ms          8                \n",
      "aten::_cat                            5.09%            19.139ms         5.14%            19.328ms         2.416ms          8                \n",
      "aten::narrow                          0.01%            36.020us         0.02%            76.592us         10.942us         7                \n",
      "aten::slice                           0.01%            29.398us         0.01%            40.572us         5.796us          7                \n",
      "aten::max_pool2d                      0.01%            44.841us         2.89%            10.878ms         2.719ms          4                \n",
      "aten::max_pool2d_with_indices         2.86%            10.736ms         2.88%            10.833ms         2.708ms          4                \n",
      "aten::contiguous                      0.01%            36.386us         0.01%            36.386us         1.516us          24               \n",
      "aten::flatten                         0.01%            41.136us         0.04%            133.761us        33.440us         4                \n",
      "aten::reshape                         0.01%            30.290us         0.02%            92.625us         23.156us         4                \n",
      "aten::view                            0.03%            115.505us        0.03%            115.505us        14.438us         8                \n",
      "aten::t                               0.07%            250.764us        0.10%            379.548us        23.722us         16               \n",
      "aten::transpose                       0.02%            75.876us         0.03%            128.784us        8.049us          16               \n",
      "aten::addmm                           11.95%           44.933ms         11.99%           45.084ms         11.271ms         4                \n",
      "aten::expand                          0.01%            23.663us         0.01%            32.948us         8.237us          4                \n",
      "aten::copy_                           0.03%            96.232us         0.03%            100.763us        25.191us         4                \n",
      "aten::log_softmax                     0.01%            40.863us         10.29%           38.676ms         9.669ms          4                \n",
      "aten::_log_softmax                    10.27%           38.599ms         10.28%           38.635ms         9.659ms          4                \n",
      "aten::nll_loss                        3.47%            13.042ms         3.55%            13.334ms         3.333ms          4                \n",
      "aten::nll_loss_forward                0.06%            236.699us        0.08%            292.146us        73.037us         4                \n",
      "aten::ones_like                       0.01%            36.391us         0.03%            124.754us        31.189us         4                \n",
      "aten::empty_like                      0.02%            73.744us         0.04%            137.864us        17.233us         8                \n",
      "aten::empty_strided                   0.01%            33.238us         0.01%            33.238us         8.310us          4                \n",
      "aten::fill_                           2.50%            9.390ms          2.50%            9.390ms          426.805us        22               \n",
      "torch::autograd::GraphRoot            0.00%            9.310us          0.00%            9.310us          2.328us          4                \n",
      "NllLossBackward                       2.32%            8.724ms          4.86%            18.271ms         4.568ms          4                \n",
      "aten::nll_loss_backward               0.03%            105.843us        2.54%            9.547ms          2.387ms          4                \n",
      "aten::zeros_like                      0.01%            34.506us         2.49%            9.373ms          2.343ms          4                \n",
      "aten::zero_                           0.02%            89.033us         2.51%            9.420ms          672.825us        14               \n",
      "aten::resize_as_                      0.01%            27.005us         0.01%            33.117us         8.279us          4                \n",
      "LogSoftmaxBackward                    2.46%            9.237ms          2.52%            9.459ms          2.365ms          4                \n",
      "aten::_log_softmax_backward_data      0.05%            187.960us        0.06%            221.510us        55.377us         4                \n",
      "AddmmBackward                         0.03%            103.194us        16.89%           63.493ms         15.873ms         4                \n",
      "aten::mm                              16.81%           63.189ms         16.81%           63.215ms         15.804ms         4                \n",
      "aten::sum                             7.28%            27.383ms         7.30%            27.455ms         6.864ms          4                \n",
      "torch::autograd::AccumulateGrad       0.01%            51.695us         0.03%            125.447us        15.681us         8                \n",
      "aten::detach                          0.00%            2.738us          0.00%            5.469us          2.735us          2                \n",
      "detach                                0.00%            2.731us          0.00%            2.731us          1.366us          2                \n",
      "TBackward                             0.00%            15.678us         0.02%            74.281us         18.570us         4                \n",
      "aten::add_                            5.03%            18.909ms         5.03%            18.909ms         1.351ms          14               \n",
      "------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "Self CPU time total: 376.005ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ls:\n",
    "    print(x.key)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c642b935ef76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Diploma/gits/.env/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Diploma/gits/.env/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    # Forward pass\n",
    "    with torch.autograd.profiler.profile() as prof:\n",
    "        outputs = model(images)\n",
    "    break\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "Name               Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  \n",
      "-----------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "aten::flatten      29.22%           3.572us          100.00%          12.226us         12.226us         1                \n",
      "aten::reshape      30.56%           3.736us          70.78%           8.654us          8.654us          1                \n",
      "aten::view         40.23%           4.918us          40.23%           4.918us          4.918us          1                \n",
      "-----------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \n",
      "Self CPU time total: 12.226us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in prof.key_averages():\n",
    "    print(k.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers  \n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from numpy.random import RandomState as R\n",
    "\n",
    "seed = 42\n",
    "\n",
    "def give(dim, n, channels):\n",
    "    ds_size = 1024\n",
    "    out_size = 10\n",
    "    if dim == 1:\n",
    "        x = R(seed).random((ds_size, n, channels))\n",
    "        x = x.reshape(x.shape[0], n, channels)\n",
    "    else:\n",
    "        x = R(seed).random((ds_size, n, n, channels))\n",
    "        x = x.reshape(x.shape[0], n, n, channels)\n",
    "    \n",
    "    y = R(seed).randint(0,out_size,ds_size)\n",
    "    y = tf.keras.utils.to_categorical(y, out_size)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def profile(model, x, y, batch, epochs):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.batch(batch)\n",
    "    \n",
    "    EPOCHS = epochs\n",
    "    prof_file = 'out_tflow.csv'\n",
    "    logdir = 'logs'\n",
    "\n",
    "    with tf.profiler.experimental.Profile(logdir):\n",
    "        model.fit(dataset, epochs = EPOCHS)\n",
    "        pass\n",
    "\n",
    "    return prof_file\n",
    "    \n",
    "class Alone:\n",
    "    def create(self):\n",
    "        model = Sequential()\n",
    "        model.add( \n",
    "            Dense(units = 32, name = 'DENSE2D')\n",
    "        )\n",
    "        model.add( Flatten(name='FLATTEN') )\n",
    "        model.add( Dense(units = 10, name='FINAL_DENSE') )\n",
    "        model.compile(loss = loss, optimizer = opt, metrics=['accuracy'])\n",
    "        self.model = model\n",
    "\n",
    "Model = Alone()\n",
    "Model.create()\n",
    "\n",
    "x,y = give(2, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof = profile(Model.model, x, y, 32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_parser as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_dir = ['intermediate_results', 'ptorch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = dp.load(os.path.join(*the_dir, 'dense.torch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in dct.items():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPU time avg (us)</th>\n",
       "      <th>CPU total (us)</th>\n",
       "      <th>Self CPU time total (us)</th>\n",
       "      <th>Number of Calls</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_complex</th>\n",
       "      <td>0.381400</td>\n",
       "      <td>1.907</td>\n",
       "      <td>1.907</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_floating_point</th>\n",
       "      <td>0.534000</td>\n",
       "      <td>2.670</td>\n",
       "      <td>2.670</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>torch::autograd::GraphRoot</th>\n",
       "      <td>3.044200</td>\n",
       "      <td>15.221</td>\n",
       "      <td>15.221</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_local_scalar_dense</th>\n",
       "      <td>1.295880</td>\n",
       "      <td>32.397</td>\n",
       "      <td>32.397</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>detach_</th>\n",
       "      <td>2.116050</td>\n",
       "      <td>42.321</td>\n",
       "      <td>42.321</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_</th>\n",
       "      <td>8.900200</td>\n",
       "      <td>44.501</td>\n",
       "      <td>44.501</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reshape</th>\n",
       "      <td>12.659600</td>\n",
       "      <td>63.298</td>\n",
       "      <td>16.237</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item</th>\n",
       "      <td>2.763480</td>\n",
       "      <td>69.087</td>\n",
       "      <td>36.690</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flatten</th>\n",
       "      <td>9.126000</td>\n",
       "      <td>91.260</td>\n",
       "      <td>27.962</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBackward</th>\n",
       "      <td>9.392400</td>\n",
       "      <td>93.924</td>\n",
       "      <td>19.168</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero_</th>\n",
       "      <td>5.753750</td>\n",
       "      <td>115.075</td>\n",
       "      <td>115.075</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nll_loss_forward</th>\n",
       "      <td>27.838800</td>\n",
       "      <td>139.194</td>\n",
       "      <td>139.194</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nll_loss</th>\n",
       "      <td>30.689000</td>\n",
       "      <td>153.445</td>\n",
       "      <td>14.251</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slice</th>\n",
       "      <td>4.371425</td>\n",
       "      <td>174.857</td>\n",
       "      <td>174.857</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>narrow</th>\n",
       "      <td>5.968750</td>\n",
       "      <td>238.750</td>\n",
       "      <td>63.893</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fill_</th>\n",
       "      <td>9.972880</td>\n",
       "      <td>249.322</td>\n",
       "      <td>249.322</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>div_</th>\n",
       "      <td>53.424200</td>\n",
       "      <td>267.121</td>\n",
       "      <td>267.121</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>torch::autograd::AccumulateGrad</th>\n",
       "      <td>13.616750</td>\n",
       "      <td>272.335</td>\n",
       "      <td>52.617</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nll_loss_backward</th>\n",
       "      <td>59.421400</td>\n",
       "      <td>297.107</td>\n",
       "      <td>297.107</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NllLossBackward</th>\n",
       "      <td>78.253400</td>\n",
       "      <td>391.267</td>\n",
       "      <td>94.160</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>view</th>\n",
       "      <td>7.147727</td>\n",
       "      <td>393.125</td>\n",
       "      <td>393.125</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unsigned short</th>\n",
       "      <td>9.764467</td>\n",
       "      <td>439.401</td>\n",
       "      <td>439.401</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add_</th>\n",
       "      <td>11.737075</td>\n",
       "      <td>469.483</td>\n",
       "      <td>469.483</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>58.664300</td>\n",
       "      <td>586.643</td>\n",
       "      <td>586.643</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_log_softmax</th>\n",
       "      <td>177.584400</td>\n",
       "      <td>887.922</td>\n",
       "      <td>887.922</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_softmax</th>\n",
       "      <td>181.437600</td>\n",
       "      <td>907.188</td>\n",
       "      <td>19.266</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_log_softmax_backward_data</th>\n",
       "      <td>187.003800</td>\n",
       "      <td>935.019</td>\n",
       "      <td>935.019</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogSoftmaxBackward</th>\n",
       "      <td>195.673200</td>\n",
       "      <td>978.366</td>\n",
       "      <td>43.347</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addmm</th>\n",
       "      <td>266.165500</td>\n",
       "      <td>2661.655</td>\n",
       "      <td>2661.655</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mm</th>\n",
       "      <td>374.088267</td>\n",
       "      <td>5611.324</td>\n",
       "      <td>5611.324</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AddmmBackward</th>\n",
       "      <td>600.156000</td>\n",
       "      <td>6001.560</td>\n",
       "      <td>156.276</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stack</th>\n",
       "      <td>1072.925700</td>\n",
       "      <td>10729.257</td>\n",
       "      <td>10729.257</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select</th>\n",
       "      <td>2.673276</td>\n",
       "      <td>13794.103</td>\n",
       "      <td>13794.103</td>\n",
       "      <td>5160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 CPU time avg (us)  CPU total (us)  \\\n",
       "Name                                                                 \n",
       "is_complex                                0.381400           1.907   \n",
       "is_floating_point                         0.534000           2.670   \n",
       "torch::autograd::GraphRoot                3.044200          15.221   \n",
       "_local_scalar_dense                       1.295880          32.397   \n",
       "detach_                                   2.116050          42.321   \n",
       "random_                                   8.900200          44.501   \n",
       "reshape                                  12.659600          63.298   \n",
       "item                                      2.763480          69.087   \n",
       "flatten                                   9.126000          91.260   \n",
       "TBackward                                 9.392400          93.924   \n",
       "zero_                                     5.753750         115.075   \n",
       "nll_loss_forward                         27.838800         139.194   \n",
       "nll_loss                                 30.689000         153.445   \n",
       "slice                                     4.371425         174.857   \n",
       "narrow                                    5.968750         238.750   \n",
       "fill_                                     9.972880         249.322   \n",
       "div_                                     53.424200         267.121   \n",
       "torch::autograd::AccumulateGrad          13.616750         272.335   \n",
       "nll_loss_backward                        59.421400         297.107   \n",
       "NllLossBackward                          78.253400         391.267   \n",
       "view                                      7.147727         393.125   \n",
       "unsigned short                            9.764467         439.401   \n",
       "add_                                     11.737075         469.483   \n",
       "sum                                      58.664300         586.643   \n",
       "_log_softmax                            177.584400         887.922   \n",
       "log_softmax                             181.437600         907.188   \n",
       "_log_softmax_backward_data              187.003800         935.019   \n",
       "LogSoftmaxBackward                      195.673200         978.366   \n",
       "addmm                                   266.165500        2661.655   \n",
       "mm                                      374.088267        5611.324   \n",
       "AddmmBackward                           600.156000        6001.560   \n",
       "stack                                  1072.925700       10729.257   \n",
       "select                                    2.673276       13794.103   \n",
       "\n",
       "                                 Self CPU time total (us)  Number of Calls  \n",
       "Name                                                                        \n",
       "is_complex                                          1.907                5  \n",
       "is_floating_point                                   2.670                5  \n",
       "torch::autograd::GraphRoot                         15.221                5  \n",
       "_local_scalar_dense                                32.397               25  \n",
       "detach_                                            42.321               20  \n",
       "random_                                            44.501                5  \n",
       "reshape                                            16.237                5  \n",
       "item                                               36.690               25  \n",
       "flatten                                            27.962               10  \n",
       "TBackward                                          19.168               10  \n",
       "zero_                                             115.075               20  \n",
       "nll_loss_forward                                  139.194                5  \n",
       "nll_loss                                           14.251                5  \n",
       "slice                                             174.857               40  \n",
       "narrow                                             63.893               40  \n",
       "fill_                                             249.322               25  \n",
       "div_                                              267.121                5  \n",
       "torch::autograd::AccumulateGrad                    52.617               20  \n",
       "nll_loss_backward                                 297.107                5  \n",
       "NllLossBackward                                    94.160                5  \n",
       "view                                              393.125               55  \n",
       "unsigned short                                    439.401               45  \n",
       "add_                                              469.483               40  \n",
       "sum                                               586.643               10  \n",
       "_log_softmax                                      887.922                5  \n",
       "log_softmax                                        19.266                5  \n",
       "_log_softmax_backward_data                        935.019                5  \n",
       "LogSoftmaxBackward                                 43.347                5  \n",
       "addmm                                            2661.655               10  \n",
       "mm                                               5611.324               15  \n",
       "AddmmBackward                                     156.276               10  \n",
       "stack                                           10729.257               10  \n",
       "select                                          13794.103             5160  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sort_values(by='CPU total (us)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
